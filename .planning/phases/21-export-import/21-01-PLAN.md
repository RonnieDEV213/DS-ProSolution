---
phase: 21-export-import
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/api/src/app/routers/export.py
  - apps/api/src/app/services/export_service.py
  - apps/api/src/app/models.py
  - apps/api/src/app/main.py
autonomous: true

must_haves:
  truths:
    - "CSV export endpoint streams data row by row without loading full dataset into memory"
    - "JSON export endpoint streams data as array elements"
    - "Excel export uses xlsxwriter constant_memory mode for large files"
    - "Background export job creates downloadable file for exports >10K rows"
    - "Export job status endpoint returns progress (row count processed)"
  artifacts:
    - path: "apps/api/src/app/routers/export.py"
      provides: "Streaming export endpoints for CSV, JSON, Excel"
      exports: ["router"]
    - path: "apps/api/src/app/services/export_service.py"
      provides: "Export generators and background job logic"
      exports: ["generate_csv_stream", "generate_json_stream", "generate_excel_file", "process_export_job"]
    - path: "apps/api/src/app/models.py"
      provides: "Export request/response models"
      contains: "ExportJobResponse"
  key_links:
    - from: "apps/api/src/app/routers/export.py"
      to: "apps/api/src/app/services/export_service.py"
      via: "import"
      pattern: "from app\\.services\\.export_service import"
    - from: "apps/api/src/app/routers/export.py"
      to: "apps/api/src/app/services/scheduler.py"
      via: "APScheduler for background jobs"
      pattern: "scheduler\\.add_job"
    - from: "apps/api/src/app/main.py"
      to: "apps/api/src/app/routers/export.py"
      via: "router registration"
      pattern: "app\\.include_router.*export"
---

<objective>
Implement backend export infrastructure with streaming endpoints for CSV, JSON, and Excel formats, plus background job support for large exports.

Purpose: Enable server-side streaming exports that don't crash on large datasets (millions of records), with background processing for exports exceeding 10K rows.

Output:
- Export router with streaming endpoints (`/export/records/csv`, `/export/records/json`, `/export/records/excel`)
- Background export endpoint (`/export/records/background`) with job status polling
- Export service with async generators for memory-efficient streaming
</objective>

<execution_context>
@C:\Users\User\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\User\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-export-import/21-CONTEXT.md
@.planning/phases/21-export-import/21-RESEARCH.md
@apps/api/src/app/routers/sync.py
@apps/api/src/app/pagination.py
@apps/api/src/app/services/scheduler.py
@apps/api/src/app/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add export models to models.py</name>
  <files>apps/api/src/app/models.py</files>
  <action>
Add export-related Pydantic models to the models file:

1. `ExportFormat` enum with values: CSV, JSON, EXCEL
2. `ExportColumn` enum or list of valid column names for export
3. `ExportRequest` model:
   - account_id: str
   - format: ExportFormat
   - columns: list[str] (optional, defaults to all)
   - status: Optional[BookkeepingStatus] (filter)
   - date_from: Optional[date]
   - date_to: Optional[date]

4. `ExportJobStatus` enum: PENDING, PROCESSING, COMPLETED, FAILED
5. `ExportJobResponse` model:
   - job_id: str
   - status: ExportJobStatus
   - row_count: Optional[int] (null until complete)
   - file_url: Optional[str] (presigned URL when complete)
   - error: Optional[str]
   - created_at: datetime
   - completed_at: Optional[datetime]

6. `ExportJobCreate` model for background export requests (same fields as ExportRequest but always returns job_id)

Place these in a new "Export/Import Models" section after the Sync Response Models section.
  </action>
  <verify>
Run `cd apps/api && python -c "from app.models import ExportFormat, ExportRequest, ExportJobResponse; print('Models OK')"` - should print "Models OK"
  </verify>
  <done>Export models defined and importable</done>
</task>

<task type="auto">
  <name>Task 2: Create export service with streaming generators</name>
  <files>apps/api/src/app/services/export_service.py</files>
  <action>
Create new export service with async generators for memory-efficient streaming:

1. **Helper function** `get_export_records_cursor_paginated()`:
   - Uses cursor pagination from sync.py pattern (_apply_cursor_filter)
   - Accepts account_id, optional status filter, optional date range
   - Yields batches of 100 records at a time
   - Computes profit/earnings/COGS for each record (like RecordResponse.from_db)

2. **CSV generator** `generate_csv_stream()`:
   - Accept account_id, columns list, optional filters
   - Yield header row first (only selected columns)
   - For each batch from paginated query:
     - Convert records to CSV rows using csv.DictWriter with StringIO
     - Yield CSV text
   - Handle currency formatting (cents to dollars for CSV readability)

3. **JSON generator** `generate_json_stream()`:
   - Accept account_id, columns list, optional filters
   - Yield `{"records": [` first
   - For each record, yield JSON object (with comma separator handling)
   - Yield `]}` at end
   - Filter columns from each record dict

4. **Excel file generator** `generate_excel_file()`:
   - Accept account_id, columns list, optional filters
   - Use xlsxwriter with constant_memory=True
   - Write to NamedTemporaryFile
   - Apply formatting: frozen header row, currency format for cents columns
   - Return file path (caller will stream)

5. **Background export job** `process_export_job()`:
   - Accept job_id
   - Load job details from export_jobs table
   - Generate file based on format
   - Upload to Supabase Storage (or local temp for now)
   - Update job status to COMPLETED with file_url
   - Handle errors: set status=FAILED with error message

6. **Computed fields helper** `compute_record_fields()`:
   - Same logic as RecordResponse.from_db for profit/earnings/COGS
   - Returns dict with computed fields added

Use existing patterns from:
- apps/api/src/app/routers/sync.py for cursor pagination
- apps/api/src/app/services/scheduler.py for APScheduler job pattern
  </action>
  <verify>
Run `cd apps/api && python -c "from app.services.export_service import generate_csv_stream, generate_json_stream, generate_excel_file; print('Service OK')"` - should print "Service OK"
  </verify>
  <done>Export service with streaming generators implemented</done>
</task>

<task type="auto">
  <name>Task 3: Create export router with streaming endpoints</name>
  <files>apps/api/src/app/routers/export.py, apps/api/src/app/main.py</files>
  <action>
Create export router with streaming endpoints:

**In apps/api/src/app/routers/export.py:**

1. `GET /export/records/csv`:
   - Query params: account_id, columns (comma-separated, optional), status, date_from, date_to
   - Use StreamingResponse with generate_csv_stream
   - Set Content-Disposition header with filename (records_{account_id}_{date}.csv)
   - Requires permission: order_tracking.read

2. `GET /export/records/json`:
   - Same params as CSV
   - Use StreamingResponse with generate_json_stream
   - Content-Type: application/json
   - Requires permission: order_tracking.read

3. `GET /export/records/excel`:
   - Same params as CSV/JSON
   - Generate Excel file, then stream file contents
   - Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
   - Requires permission: order_tracking.read

4. `POST /export/records/background`:
   - Request body: ExportJobCreate (account_id, format, columns, filters)
   - Check record count first - if <10K rows, return error suggesting streaming endpoint
   - Create export_jobs table row with status=PENDING
   - Schedule job with APScheduler (similar to collection scheduler)
   - Return job_id

5. `GET /export/jobs/{job_id}`:
   - Return ExportJobResponse with current status
   - If completed, include presigned URL for download
   - Requires permission: order_tracking.read

6. `GET /export/jobs`:
   - List user's export jobs (paginated)
   - Ordered by created_at DESC
   - Requires permission: order_tracking.read

**In apps/api/src/app/main.py:**
- Import and register export router: `app.include_router(export.router, prefix="/export", tags=["export"])`

**Column list for exports** (based on RecordSyncItem + computed):
- id, ebay_order_id, sale_date, item_name, qty
- sale_price_cents, ebay_fees_cents, earnings_net_cents
- amazon_price_cents, amazon_tax_cents, amazon_shipping_cents, cogs_total_cents
- amazon_order_id, status, return_label_cost_cents, profit_cents
- order_remark, service_remark (if user has access)
  </action>
  <verify>
1. Run `cd apps/api && python -c "from app.routers.export import router; print('Router OK')"`
2. Start API: `cd apps/api && uvicorn app.main:app --reload --app-dir src &`
3. Check docs: `curl http://localhost:8000/docs | grep -o '/export'` should show export endpoints
  </verify>
  <done>Export router registered and streaming endpoints available</done>
</task>

</tasks>

<verification>
1. CSV streaming: `curl "http://localhost:8000/export/records/csv?account_id={id}" -H "Authorization: Bearer {token}" | head -20` shows CSV rows streaming
2. JSON streaming: `curl "http://localhost:8000/export/records/json?account_id={id}" -H "Authorization: Bearer {token}" | head -20` shows JSON array
3. Excel endpoint: `curl "http://localhost:8000/export/records/excel?account_id={id}" -H "Authorization: Bearer {token}" -o test.xlsx` creates valid Excel file
4. Background job: `POST /export/records/background` returns job_id, `GET /export/jobs/{id}` shows status
5. Memory check: Export endpoint with large dataset doesn't spike server memory (streaming works)
</verification>

<success_criteria>
- All three streaming endpoints (CSV, JSON, Excel) return data without loading full dataset into memory
- Background export job creates file and updates status to COMPLETED
- Export includes computed fields (profit, earnings, COGS)
- Column selection filters which fields appear in export
- FastAPI docs show all export endpoints
</success_criteria>

<output>
After completion, create `.planning/phases/21-export-import/21-01-SUMMARY.md`
</output>
