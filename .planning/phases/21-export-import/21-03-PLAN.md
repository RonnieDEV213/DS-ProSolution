---
phase: 21-export-import
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/api/src/app/routers/import.py
  - apps/api/src/app/services/import_service.py
  - apps/api/src/app/models.py
  - apps/api/src/app/main.py
  - supabase/migrations/XXX_import_batch_tracking.sql
autonomous: true

must_haves:
  truths:
    - "Import validation endpoint parses first 100 rows and returns preview with errors"
    - "Import validation suggests column mapping based on header similarity"
    - "Import commit creates all records with shared batch_id for tracking"
    - "Import rollback soft-deletes all records in batch"
    - "Rollback warns if records were modified since import"
    - "Batches older than 24 hours cannot be rolled back"
  artifacts:
    - path: "apps/api/src/app/routers/import.py"
      provides: "Import endpoints for validation, commit, rollback"
      exports: ["router"]
    - path: "apps/api/src/app/services/import_service.py"
      provides: "Import validation, column mapping, batch tracking logic"
      exports: ["validate_import_file", "suggest_column_mapping", "commit_import", "rollback_import"]
    - path: "apps/api/src/app/models.py"
      provides: "Import request/response models"
      contains: "ImportValidationResponse"
    - path: "supabase/migrations"
      provides: "import_batches table and import_batch_id column"
      contains: "import_batches"
  key_links:
    - from: "apps/api/src/app/routers/import.py"
      to: "apps/api/src/app/services/import_service.py"
      via: "import"
      pattern: "from app\\.services\\.import_service import"
    - from: "apps/api/src/app/main.py"
      to: "apps/api/src/app/routers/import.py"
      via: "router registration"
      pattern: "app\\.include_router.*import"
---

<objective>
Implement backend import infrastructure with validation preview, smart column mapping, batch tracking for rollback, and 24-hour rollback window enforcement.

Purpose: Enable safe data imports with validation before commit, automatic column mapping, and undo capability within 24 hours.

Output:
- Import router with validation, commit, rollback endpoints
- Import service with file parsing, validation, column mapping
- Database migration for import batch tracking
- Rollback logic with modified-record detection
</objective>

<execution_context>
@C:\Users\User\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\User\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-export-import/21-CONTEXT.md
@.planning/phases/21-export-import/21-RESEARCH.md
@apps/api/src/app/routers/sync.py
@apps/api/src/app/models.py
@apps/api/src/app/services/scheduler.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create database migration for import batch tracking</name>
  <files>supabase/migrations/XXX_import_batch_tracking.sql</files>
  <action>
Create new migration file with timestamp prefix (e.g., 20260125000000_import_batch_tracking.sql):

```sql
-- Import batch tracking for rollback capability
-- Per Phase 21 CONTEXT.md: 24-hour rollback window, warn about modified records

-- Table to track import batches
CREATE TABLE import_batches (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES auth.users(id),
    org_id UUID NOT NULL REFERENCES organizations(id),
    account_id UUID NOT NULL REFERENCES accounts(id),
    filename TEXT NOT NULL,
    row_count INTEGER NOT NULL DEFAULT 0,
    format TEXT NOT NULL DEFAULT 'csv', -- csv, json, excel
    column_mapping JSONB, -- Store the mapping used for this import
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    can_rollback BOOLEAN NOT NULL DEFAULT TRUE,
    rolled_back_at TIMESTAMPTZ -- Set when rollback executed
);

-- Index for listing user's import history
CREATE INDEX idx_import_batches_user_created ON import_batches(user_id, created_at DESC);
CREATE INDEX idx_import_batches_org ON import_batches(org_id);

-- Add import_batch_id to bookkeeping_records for tracking
ALTER TABLE bookkeeping_records
ADD COLUMN import_batch_id UUID REFERENCES import_batches(id);

-- Index for efficient batch operations (only for imported records)
CREATE INDEX idx_records_import_batch ON bookkeeping_records(import_batch_id)
WHERE import_batch_id IS NOT NULL;

-- RLS policies for import_batches
ALTER TABLE import_batches ENABLE ROW LEVEL SECURITY;

-- Users can see their own org's import batches
CREATE POLICY "Users can view org import batches"
ON import_batches
FOR SELECT
USING (
    org_id IN (
        SELECT org_id FROM memberships
        WHERE user_id = auth.uid()
        AND status = 'active'
    )
);

-- Users with order_tracking.write can create import batches
CREATE POLICY "Users can create import batches with permission"
ON import_batches
FOR INSERT
WITH CHECK (
    org_id IN (
        SELECT org_id FROM memberships
        WHERE user_id = auth.uid()
        AND status = 'active'
    )
);

-- Users with order_tracking.write can update import batches (for rollback)
CREATE POLICY "Users can update import batches with permission"
ON import_batches
FOR UPDATE
USING (
    org_id IN (
        SELECT org_id FROM memberships
        WHERE user_id = auth.uid()
        AND status = 'active'
    )
);

-- Function to disable rollback for old batches (run by scheduler)
CREATE OR REPLACE FUNCTION disable_old_import_rollbacks()
RETURNS void AS $$
BEGIN
    UPDATE import_batches
    SET can_rollback = FALSE
    WHERE can_rollback = TRUE
    AND created_at < NOW() - INTERVAL '24 hours';
END;
$$ LANGUAGE plpgsql;

-- Comment documenting the 24-hour rollback policy
COMMENT ON TABLE import_batches IS 'Tracks import operations for rollback capability. Rollback available for 24 hours after import.';
```

Note: Actual timestamp in filename should be generated at execution time.
  </action>
  <verify>
Apply migration to local Supabase: `cd supabase && npx supabase db push` or run migration manually
Verify table exists: Check in Supabase Studio or run `SELECT * FROM import_batches LIMIT 1;`
  </verify>
  <done>Import batch tracking table and column created with RLS policies</done>
</task>

<task type="auto">
  <name>Task 2: Add import models and create import service</name>
  <files>apps/api/src/app/models.py, apps/api/src/app/services/import_service.py</files>
  <action>
**Add to apps/api/src/app/models.py:**

```python
# ============================================================
# Import Models
# ============================================================


class ImportFormat(str, Enum):
    CSV = "csv"
    JSON = "json"
    EXCEL = "excel"


class ImportValidationError(BaseModel):
    """Single validation error for a row."""
    row: int  # Excel row number (1-indexed + header)
    field: str
    message: str


class ImportPreviewRow(BaseModel):
    """Single row in import preview."""
    row_number: int
    data: dict  # Mapped column data
    is_valid: bool
    errors: list[ImportValidationError]


class ImportValidationResponse(BaseModel):
    """Response from import validation endpoint."""
    preview: list[ImportPreviewRow]  # First 100 rows
    errors: list[ImportValidationError]  # All errors from preview
    total_rows: int
    valid_rows: int
    suggested_mapping: dict[str, str]  # CSV header -> DB field


class ImportCommitRequest(BaseModel):
    """Request to commit validated import."""
    account_id: str
    filename: str
    format: ImportFormat
    column_mapping: dict[str, str]  # User-confirmed mapping
    # File data passed separately via form data


class ImportCommitResponse(BaseModel):
    """Response from import commit."""
    batch_id: str
    rows_imported: int


class ImportBatchResponse(BaseModel):
    """Import batch details for history/rollback."""
    id: str
    account_id: str
    filename: str
    row_count: int
    format: str
    created_at: datetime
    can_rollback: bool
    rolled_back_at: Optional[datetime] = None


class ImportBatchListResponse(BaseModel):
    """List of import batches."""
    batches: list[ImportBatchResponse]


class ImportRollbackWarning(BaseModel):
    """Warning about modified records before rollback."""
    warning: str
    modified_count: int
    modified_record_ids: list[str]
    requires_confirmation: bool


class ImportRollbackResponse(BaseModel):
    """Response from rollback (success or warning)."""
    success: bool = False
    rows_deleted: int = 0
    warning: Optional[ImportRollbackWarning] = None
```

**Create apps/api/src/app/services/import_service.py:**

Import service with:

1. **suggest_column_mapping(headers: list[str]) -> dict[str, str]:**
   - Use difflib.SequenceMatcher for similarity matching
   - Map common variations: "Order ID" -> "ebay_order_id", "Date" -> "sale_date", etc.
   - Return dict mapping CSV header to DB field name
   - 70% similarity threshold for fuzzy matching

2. **validate_import_file(file_content: bytes, format: ImportFormat) -> tuple[DataFrame, int]:**
   - Parse file using pandas (read_csv, read_json, read_excel)
   - Return (preview DataFrame of first 100 rows, total row count)
   - Handle encoding detection for CSV
   - Raise ValueError with message for parse errors

3. **validate_rows(df: DataFrame, mapping: dict[str, str]) -> list[ImportPreviewRow]:**
   - For each row in preview DataFrame:
     - Map columns using provided mapping
     - Validate required fields (ebay_order_id, sale_date, item_name, sale_price_cents)
     - Validate data types (date format, numeric cents fields)
     - Validate enum values (status must be valid BookkeepingStatus)
   - Return list of ImportPreviewRow with validation results

4. **commit_import(supabase, account_id: str, user_id: str, org_id: str, data: list[dict], filename: str, format: str, mapping: dict) -> str:**
   - Create import_batch record
   - Insert all records with import_batch_id
   - Use all-or-nothing transaction (if any row fails, entire import fails)
   - Return batch_id

5. **check_rollback_eligibility(supabase, batch_id: str) -> tuple[bool, Optional[ImportRollbackWarning]]:**
   - Check if batch exists and can_rollback is True
   - Check if batch is within 24-hour window
   - Check for modified records (updated_at != created_at)
   - Return (can_rollback, warning_if_modified)

6. **rollback_import(supabase, batch_id: str, force: bool = False) -> int:**
   - If not force, check for modified records and raise if found
   - Soft-delete all records with import_batch_id (set deleted_at)
   - Update batch: can_rollback=False, rolled_back_at=NOW()
   - Return count of deleted records

Expected DB fields for records (from RecordCreate model):
- account_id, ebay_order_id, sale_date, item_name, qty, sale_price_cents
- ebay_fees_cents, amazon_price_cents, amazon_tax_cents, amazon_shipping_cents
- amazon_order_id, order_remark, status
  </action>
  <verify>
Run `cd apps/api && python -c "from app.services.import_service import suggest_column_mapping, validate_import_file; print('Import service OK')"` - should print "Import service OK"
  </verify>
  <done>Import models and service created with validation, column mapping, and rollback logic</done>
</task>

<task type="auto">
  <name>Task 3: Create import router with endpoints</name>
  <files>apps/api/src/app/routers/import.py, apps/api/src/app/main.py</files>
  <action>
**Create apps/api/src/app/routers/import.py:**

```python
"""Import endpoints for data import with validation and rollback."""

import logging
from datetime import datetime, timezone
from typing import Optional

from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile, Query

from app.auth import require_permission_key
from app.database import get_supabase_for_user
from app.models import (
    ImportFormat,
    ImportValidationResponse,
    ImportCommitResponse,
    ImportBatchResponse,
    ImportBatchListResponse,
    ImportRollbackResponse,
)
from app.services.import_service import (
    suggest_column_mapping,
    validate_import_file,
    validate_rows,
    commit_import,
    check_rollback_eligibility,
    rollback_import,
)

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/import", tags=["import"])
```

**Endpoints:**

1. `POST /import/records/validate`:
   - Accept: UploadFile (file) + format (query param)
   - Parse file using validate_import_file
   - Get suggested column mapping
   - Validate rows with suggested mapping
   - Return ImportValidationResponse with preview, errors, suggested_mapping
   - Requires permission: order_tracking.write

2. `POST /import/records/commit`:
   - Accept: UploadFile (file) + Form data (account_id, filename, format, column_mapping as JSON string)
   - Parse full file
   - Validate ALL rows with provided mapping (not just preview)
   - If any row invalid: return 400 with errors (all-or-nothing)
   - If all valid: commit to database with batch tracking
   - Return ImportCommitResponse with batch_id and row count
   - Requires permission: order_tracking.write

3. `GET /import/batches`:
   - Query params: account_id (optional), limit (default 20)
   - Return ImportBatchListResponse with user's import history
   - Ordered by created_at DESC
   - Requires permission: order_tracking.read

4. `GET /import/batches/{batch_id}`:
   - Return single ImportBatchResponse
   - Requires permission: order_tracking.read

5. `POST /import/batches/{batch_id}/rollback`:
   - Query param: force (boolean, default false)
   - Check eligibility (24h window, can_rollback flag)
   - If modified records exist and not force: return warning with modified record IDs
   - If force or no modified records: execute rollback
   - Return ImportRollbackResponse
   - Requires permission: order_tracking.write

6. `POST /import/batches/{batch_id}/disable-rollback`:
   - Manually disable rollback for a batch (if user wants to prevent accidental rollback)
   - Sets can_rollback = False
   - Requires permission: order_tracking.write

**Register in apps/api/src/app/main.py:**
```python
from app.routers import import as import_router
# ...
app.include_router(import_router.router)
```

Note: Use `import as import_router` since `import` is a Python keyword.
  </action>
  <verify>
1. Run `cd apps/api && python -c "from app.routers.import import router; print('Router OK')"`
2. Start API: `cd apps/api && uvicorn app.main:app --reload --app-dir src &`
3. Check docs: `curl http://localhost:8000/docs | grep -o '/import'` should show import endpoints
  </verify>
  <done>Import router registered with validation, commit, and rollback endpoints</done>
</task>

</tasks>

<verification>
1. Validation endpoint: Upload CSV with mixed valid/invalid rows, verify preview shows errors
2. Column mapping: Upload CSV with non-standard headers, verify suggested mapping is reasonable
3. Commit: Import valid file, verify all records have same import_batch_id
4. Rollback: Rollback recent import, verify records are soft-deleted
5. Rollback warning: Modify an imported record, attempt rollback, verify warning returned
6. 24-hour enforcement: Import, wait (or manually set created_at), verify rollback blocked
7. All-or-nothing: Import file with one bad row, verify no records created
</verification>

<success_criteria>
- Import validation parses first 100 rows and identifies errors
- Column mapping suggests correct mappings for common header variations
- Import commit uses all-or-nothing transaction
- Rollback soft-deletes records and updates batch status
- Rollback warns about modified records before proceeding
- Batches older than 24 hours cannot be rolled back
</success_criteria>

<output>
After completion, create `.planning/phases/21-export-import/21-03-SUMMARY.md`
</output>
