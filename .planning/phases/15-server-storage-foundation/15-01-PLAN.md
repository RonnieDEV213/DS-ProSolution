---
phase: 15-server-storage-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/api/migrations/046_sync_infrastructure_columns.sql
  - apps/api/migrations/047_sync_infrastructure_indexes.sql
  - apps/api/migrations/048_sync_purge_job.sql
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Cursor queries on bookkeeping_records use index scan (not seq scan)"
    - "Cursor queries on accounts use index scan (not seq scan)"
    - "Cursor queries on sellers use index scan (not seq scan)"
    - "Row modifications auto-update the updated_at timestamp"
    - "Soft-deleted rows remain queryable for sync detection"
    - "Records deleted >30 days ago are automatically purged"
  artifacts:
    - path: "apps/api/migrations/046_sync_infrastructure_columns.sql"
      provides: "updated_at and deleted_at columns with triggers"
      contains: "CREATE TRIGGER"
    - path: "apps/api/migrations/047_sync_infrastructure_indexes.sql"
      provides: "Composite cursor indexes"
      contains: "CREATE INDEX CONCURRENTLY"
    - path: "apps/api/migrations/048_sync_purge_job.sql"
      provides: "pg_cron daily purge job"
      contains: "cron.schedule"
  key_links:
    - from: "046_sync_infrastructure_columns.sql"
      to: "public.update_updated_at()"
      via: "Trigger uses existing function from 001_auth_schema.sql"
      pattern: "EXECUTE FUNCTION public\\.update_updated_at"
    - from: "047_sync_infrastructure_indexes.sql"
      to: "046_sync_infrastructure_columns.sql"
      via: "Indexes reference updated_at column added in 046"
      pattern: "updated_at DESC"
    - from: "048_sync_purge_job.sql"
      to: "046_sync_infrastructure_columns.sql"
      via: "Purge job references deleted_at column added in 046"
      pattern: "deleted_at.*INTERVAL"
---

<objective>
Create database migrations for sync infrastructure: composite indexes for cursor pagination, updated_at triggers for change tracking, and soft deletes with automated purge.

Purpose: Enable efficient cursor-based queries that execute in constant time regardless of page depth, supporting millions of records without performance degradation.

Output: Three SQL migration files ready for execution in Supabase SQL editor.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-server-storage-foundation/15-CONTEXT.md
@.planning/phases/15-server-storage-foundation/15-RESEARCH.md
@apps/api/migrations/001_auth_schema.sql
@apps/api/migrations/037_collection_infrastructure.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create column and trigger migration (046)</name>
  <files>apps/api/migrations/046_sync_infrastructure_columns.sql</files>
  <action>
Create migration `046_sync_infrastructure_columns.sql` that adds sync infrastructure columns:

**For `bookkeeping_records`:**
- Add `updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()` (backfills existing rows)
- Add `deleted_at TIMESTAMPTZ` (nullable, NULL means not deleted)
- Create trigger using existing `public.update_updated_at()` function (from 001_auth_schema.sql)

**For `accounts`:**
- Add `updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()` (backfills existing rows)
- Add `deleted_at TIMESTAMPTZ`
- Create trigger using `public.update_updated_at()`

**For `sellers`:**
- Has `updated_at` column but NO trigger - add trigger using `public.update_updated_at()`
- Add `deleted_at TIMESTAMPTZ`

**Pattern to follow (from 001_auth_schema.sql):**
```sql
-- Add columns with IF NOT EXISTS for idempotency
ALTER TABLE public.{table}
  ADD COLUMN IF NOT EXISTS updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  ADD COLUMN IF NOT EXISTS deleted_at TIMESTAMPTZ;

-- Create trigger (drop first for idempotency)
DROP TRIGGER IF EXISTS {table}_updated_at ON public.{table};
CREATE TRIGGER {table}_updated_at
  BEFORE UPDATE ON public.{table}
  FOR EACH ROW EXECUTE FUNCTION public.update_updated_at();
```

**Important:**
- Use `ADD COLUMN IF NOT EXISTS` for idempotency
- `DEFAULT NOW()` backfills existing rows automatically
- Trigger must be `BEFORE UPDATE` to modify NEW.updated_at
- The soft delete mechanism works because updating `deleted_at` is an UPDATE, which triggers `updated_at` to update - clients detect deletions via `updated_at` in sync queries
- Include migration header comment with filename, purpose, and phase reference
  </action>
  <verify>
- File exists at `apps/api/migrations/046_sync_infrastructure_columns.sql`
- Contains ALTER TABLE for all 3 tables (bookkeeping_records, accounts, sellers)
- Contains CREATE TRIGGER for all 3 tables
- Uses existing `public.update_updated_at()` function (not creating new one)
  </verify>
  <done>
Migration file creates updated_at column + trigger and deleted_at column for bookkeeping_records, accounts, and sellers tables.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create composite index migration (047)</name>
  <files>apps/api/migrations/047_sync_infrastructure_indexes.sql</files>
  <action>
Create migration `047_sync_infrastructure_indexes.sql` that adds composite indexes for cursor-based pagination:

**Index Strategy (from RESEARCH.md):**
- Use `CREATE INDEX CONCURRENTLY` for all tables (bookkeeping_records, accounts, sellers are active tables)
- Composite key: `(account_id, updated_at DESC, id DESC)` for account-scoped tables
- For accounts table: `(org_id, updated_at DESC, id DESC)` since accounts are org-scoped, not account-scoped

**Cursor Index for `bookkeeping_records`:**
```sql
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_bookkeeping_records_cursor
  ON public.bookkeeping_records (account_id, updated_at DESC, id DESC);
```

**Cursor Index for `accounts`:**
```sql
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_accounts_cursor
  ON public.accounts (org_id, updated_at DESC, id DESC);
```

**Cursor Index for `sellers`:**
```sql
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sellers_cursor
  ON public.sellers (org_id, updated_at DESC, id DESC);
```

**Partial indexes for active (non-deleted) records:**
Add partial indexes that filter out deleted rows for common queries:
```sql
CREATE INDEX IF NOT EXISTS idx_bookkeeping_records_active
  ON public.bookkeeping_records (account_id, sale_date DESC, id DESC)
  WHERE deleted_at IS NULL;
```
(Similar pattern for accounts and sellers)

**Important:**
- CONCURRENTLY prevents write locks during index creation
- `IF NOT EXISTS` for idempotency (CONCURRENTLY supports this in PostgreSQL 14+)
- DESC ordering matches typical "newest first" query pattern
- Include comment explaining cursor query pattern these indexes support
  </action>
  <verify>
- File exists at `apps/api/migrations/047_sync_infrastructure_indexes.sql`
- Contains `CREATE INDEX CONCURRENTLY` for cursor indexes
- Contains partial indexes with `WHERE deleted_at IS NULL`
- All 3 tables have cursor indexes
  </verify>
  <done>
Migration file creates composite cursor indexes and partial active-record indexes for all 3 syncable tables.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create purge job migration (048)</name>
  <files>apps/api/migrations/048_sync_purge_job.sql</files>
  <action>
Create migration `048_sync_purge_job.sql` that sets up automated purge of old soft-deleted records:

**Enable pg_cron extension:**
```sql
CREATE EXTENSION IF NOT EXISTS pg_cron WITH SCHEMA extensions;
```

**Create purge function:**
```sql
CREATE OR REPLACE FUNCTION public.purge_soft_deleted_records()
RETURNS void
LANGUAGE plpgsql
SET search_path = ''
AS $$
BEGIN
  -- Permanently delete records soft-deleted > 30 days ago
  -- Per CONTEXT.md: 30-day retention before permanent purge

  DELETE FROM public.bookkeeping_records
  WHERE deleted_at IS NOT NULL
    AND deleted_at < NOW() - INTERVAL '30 days';

  DELETE FROM public.accounts
  WHERE deleted_at IS NOT NULL
    AND deleted_at < NOW() - INTERVAL '30 days';

  DELETE FROM public.sellers
  WHERE deleted_at IS NOT NULL
    AND deleted_at < NOW() - INTERVAL '30 days';
END;
$$;
```

**Schedule daily purge job:**
```sql
-- Schedule at 3:00 AM UTC (off-peak per CONTEXT.md)
-- Use unschedule first for idempotency
SELECT cron.unschedule('purge-soft-deleted-records') WHERE EXISTS (
  SELECT 1 FROM cron.job WHERE jobname = 'purge-soft-deleted-records'
);

SELECT cron.schedule(
  'purge-soft-deleted-records',
  '0 3 * * *',  -- 3:00 AM UTC daily
  $$ SELECT public.purge_soft_deleted_records(); $$
);
```

**Important:**
- pg_cron uses UTC timezone on Supabase
- 30-day retention per CONTEXT.md decision
- No archive needed per CONTEXT.md decision
- Function uses `SET search_path = ''` for security (codebase convention)
- Include comments documenting retention policy and schedule
  </action>
  <verify>
- File exists at `apps/api/migrations/048_sync_purge_job.sql`
- Contains `CREATE EXTENSION IF NOT EXISTS pg_cron`
- Contains purge function with 30-day interval for all 3 tables
- Contains `cron.schedule` call with daily 3 AM schedule
  </verify>
  <done>
Migration file creates pg_cron purge job that permanently deletes soft-deleted records older than 30 days, running daily at 3 AM UTC.
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify:

1. **File structure:**
   ```
   apps/api/migrations/
   ├── 046_sync_infrastructure_columns.sql
   ├── 047_sync_infrastructure_indexes.sql
   └── 048_sync_purge_job.sql
   ```

2. **Migration content checks:**
   - 046: Contains updated_at + deleted_at columns for 3 tables, triggers for 3 tables
   - 047: Contains CONCURRENTLY indexes for 3 tables, partial indexes for 3 tables
   - 048: Contains pg_cron extension, purge function, cron.schedule call

3. **Codebase consistency:**
   - Triggers use existing `public.update_updated_at()` function
   - SQL style matches existing migrations (comments, naming)
   - SET search_path = '' on new functions
</verification>

<success_criteria>
1. Three migration files exist in apps/api/migrations/
2. Each migration is idempotent (can be run multiple times safely)
3. Migrations follow codebase conventions (header comments, naming, search_path security)
4. Coverage: bookkeeping_records, accounts, sellers tables all have:
   - updated_at column with auto-update trigger
   - deleted_at column for soft deletes
   - Composite cursor index
   - Partial index for active records
5. Purge job configured for 30-day retention at 3 AM UTC daily
</success_criteria>

<output>
After completion, create `.planning/phases/15-server-storage-foundation/15-01-SUMMARY.md`
</output>
