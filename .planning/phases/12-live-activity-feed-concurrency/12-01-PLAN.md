---
phase: 12-live-activity-feed-concurrency
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/api/migrations/045_seller_snapshots.sql
  - apps/api/src/app/services/parallel_runner.py
  - apps/api/src/app/services/activity_stream.py
  - apps/api/src/app/routers/collection.py
  - apps/api/src/app/models.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "SSE endpoint streams activity events in real-time"
    - "Activity queue accepts events from collection workers"
    - "Parallel runner distributes work across 5 workers using asyncio.Queue"
    - "Database supports seller_count_snapshot on collection_runs and seller_audit_log"
  artifacts:
    - path: "apps/api/migrations/045_seller_snapshots.sql"
      provides: "seller_count_snapshot columns on collection_runs and seller_audit_log"
      contains: "seller_count_snapshot"
    - path: "apps/api/src/app/services/parallel_runner.py"
      provides: "ParallelCollectionRunner with work queue and shared failure counter"
      exports: ["ParallelCollectionRunner"]
    - path: "apps/api/src/app/services/activity_stream.py"
      provides: "In-memory activity queue manager keyed by run_id"
      exports: ["ActivityStreamManager", "get_activity_stream"]
    - path: "apps/api/src/app/routers/collection.py"
      provides: "SSE endpoint /runs/{run_id}/activity"
      contains: "activity_stream"
  key_links:
    - from: "apps/api/src/app/routers/collection.py"
      to: "apps/api/src/app/services/activity_stream.py"
      via: "get_activity_stream import"
      pattern: "from app.services.activity_stream import"
    - from: "apps/api/src/app/services/parallel_runner.py"
      to: "asyncio.Queue"
      via: "work queue"
      pattern: "asyncio\\.Queue"
---

<objective>
Create backend infrastructure for parallel collection execution and real-time activity streaming.

Purpose: Enable 5 parallel workers for collection and SSE-based activity feed streaming.
Output: Migration for seller snapshots, ParallelCollectionRunner class, ActivityStreamManager, and SSE endpoint.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-live-activity-feed-concurrency/12-CONTEXT.md
@.planning/phases/12-live-activity-feed-concurrency/12-RESEARCH.md
@apps/api/src/app/services/collection.py
@apps/api/src/app/services/db_utils.py
@apps/api/src/app/routers/collection.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create seller snapshot migration</name>
  <files>apps/api/migrations/045_seller_snapshots.sql</files>
  <action>
Create migration to add seller_count_snapshot column to both tables:

```sql
-- Migration: 045_seller_snapshots.sql
-- Purpose: Store seller count snapshot for history "sellers at this point" display

-- Add snapshot count to collection_runs (for collection run entries)
ALTER TABLE collection_runs
ADD COLUMN IF NOT EXISTS seller_count_snapshot INTEGER;

-- Add snapshot count to seller_audit_log (for manual edit entries)
ALTER TABLE seller_audit_log
ADD COLUMN IF NOT EXISTS seller_count_snapshot INTEGER;

-- Comments
COMMENT ON COLUMN collection_runs.seller_count_snapshot IS 'Total seller count in org when this run completed';
COMMENT ON COLUMN seller_audit_log.seller_count_snapshot IS 'Total seller count in org after this change';
```

This stores the seller count at the moment of completion for reliable "sellers at this point" display.
  </action>
  <verify>File exists at apps/api/migrations/045_seller_snapshots.sql with ADD COLUMN statements</verify>
  <done>Migration file created with seller_count_snapshot columns for both tables</done>
</task>

<task type="auto">
  <name>Task 2: Create ParallelCollectionRunner class</name>
  <files>apps/api/src/app/services/parallel_runner.py</files>
  <action>
Create new file with ParallelCollectionRunner class implementing work-stealing queue pattern:

```python
"""Parallel collection runner using asyncio work-stealing queue.

Provides concurrent execution of collection tasks with:
- asyncio.Queue for work distribution (atomic task claiming)
- Shared failure counter with asyncio.Lock
- Activity event emission for SSE streaming
- Configurable worker count (default 5)
"""

import asyncio
import logging
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Callable, Awaitable, TypeVar

logger = logging.getLogger(__name__)

T = TypeVar('T')
R = TypeVar('R')

MAX_WORKERS = 5
MAX_CONSECUTIVE_FAILURES = 5


@dataclass
class ActivityEvent:
    """Activity event for SSE streaming."""
    id: str
    timestamp: str
    worker_id: int
    phase: str  # "amazon" or "ebay"
    action: str  # "fetching", "found", "error", "rate_limited", "complete"
    category: str | None = None
    product_name: str | None = None
    seller_found: str | None = None
    new_sellers_count: int | None = None
    error_message: str | None = None

    def to_dict(self) -> dict[str, Any]:
        return {k: v for k, v in self.__dict__.items() if v is not None}


class CollectionPausedException(Exception):
    """Raised when collection should pause due to consecutive failures."""
    pass


class ParallelCollectionRunner:
    """
    Orchestrates parallel collection with work-stealing queue pattern.

    Usage:
        runner = ParallelCollectionRunner(
            on_activity=lambda event: activity_queue.put_nowait(event)
        )
        results = await runner.run(tasks, process_task_fn)
    """

    def __init__(
        self,
        max_workers: int = MAX_WORKERS,
        on_activity: Callable[[ActivityEvent], None] | None = None,
    ):
        self.max_workers = max_workers
        self.on_activity = on_activity
        self.work_queue: asyncio.Queue = asyncio.Queue()
        self.consecutive_failures = 0
        self.failure_lock = asyncio.Lock()
        self._cancelled = False

    def cancel(self):
        """Signal workers to stop processing."""
        self._cancelled = True

    async def emit_activity(self, event: ActivityEvent):
        """Emit activity event if callback registered."""
        if self.on_activity:
            try:
                self.on_activity(event)
            except Exception as e:
                logger.warning(f"Failed to emit activity: {e}")

    async def handle_failure(self, worker_id: int, error: str) -> bool:
        """
        Handle task failure. Returns True if should pause collection.

        Thread-safe via asyncio.Lock.
        """
        async with self.failure_lock:
            self.consecutive_failures += 1
            logger.warning(f"Worker {worker_id} failure: {error} (consecutive: {self.consecutive_failures})")

            if self.consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                logger.error(f"Max consecutive failures reached ({MAX_CONSECUTIVE_FAILURES})")
                return True
            return False

    async def reset_failures(self):
        """Reset failure counter on success."""
        async with self.failure_lock:
            self.consecutive_failures = 0

    async def worker(
        self,
        worker_id: int,
        process_task: Callable[[T, int], Awaitable[R]],
        phase: str,
    ) -> list[R]:
        """
        Worker pulls tasks from queue until poison pill received.

        Args:
            worker_id: Worker identifier (1-5)
            process_task: Async function that processes a single task
            phase: Current phase ("amazon" or "ebay")

        Returns:
            List of results from processed tasks
        """
        results: list[R] = []

        while not self._cancelled:
            try:
                task = await asyncio.wait_for(self.work_queue.get(), timeout=0.5)
            except asyncio.TimeoutError:
                # Check if queue is empty and all workers should exit
                if self.work_queue.empty():
                    break
                continue

            if task is None:  # Poison pill
                self.work_queue.task_done()
                break

            try:
                result = await process_task(task, worker_id)
                results.append(result)
                await self.reset_failures()

            except CollectionPausedException:
                self.work_queue.task_done()
                raise

            except Exception as e:
                should_pause = await self.handle_failure(worker_id, str(e))
                if should_pause:
                    self.work_queue.task_done()
                    raise CollectionPausedException(f"Max failures reached: {e}")

            finally:
                self.work_queue.task_done()

        return results

    async def run(
        self,
        tasks: list[T],
        process_task: Callable[[T, int], Awaitable[R]],
        phase: str = "collection",
    ) -> list[R]:
        """
        Execute tasks in parallel using work-stealing queue.

        Args:
            tasks: List of tasks to process
            process_task: Async function(task, worker_id) -> result
            phase: Phase name for activity events

        Returns:
            Combined results from all workers
        """
        if not tasks:
            return []

        # Reset state
        self._cancelled = False
        self.consecutive_failures = 0
        self.work_queue = asyncio.Queue()

        # Populate queue
        for task in tasks:
            await self.work_queue.put(task)

        # Add poison pills for clean shutdown
        for _ in range(self.max_workers):
            await self.work_queue.put(None)

        # Start workers
        worker_tasks = [
            asyncio.create_task(self.worker(i + 1, process_task, phase))
            for i in range(self.max_workers)
        ]

        # Wait for all workers
        try:
            results_per_worker = await asyncio.gather(*worker_tasks, return_exceptions=True)
        except Exception as e:
            logger.error(f"Parallel run failed: {e}")
            self.cancel()
            raise

        # Flatten results, filtering out exceptions
        all_results: list[R] = []
        for result in results_per_worker:
            if isinstance(result, list):
                all_results.extend(result)
            elif isinstance(result, Exception):
                logger.warning(f"Worker exception: {result}")

        return all_results


def create_activity_event(
    worker_id: int,
    phase: str,
    action: str,
    **kwargs,
) -> ActivityEvent:
    """Helper to create ActivityEvent with auto-generated id and timestamp."""
    import uuid
    return ActivityEvent(
        id=str(uuid.uuid4()),
        timestamp=datetime.now(timezone.utc).isoformat(),
        worker_id=worker_id,
        phase=phase,
        action=action,
        **kwargs,
    )
```

Key features:
- asyncio.Queue for atomic work distribution (no two workers claim same task)
- asyncio.Lock for thread-safe failure counter
- Poison pill pattern for clean shutdown
- Activity event emission for SSE streaming
- Configurable but defaults to 5 workers
  </action>
  <verify>File exists with ParallelCollectionRunner class, has asyncio.Queue, asyncio.Lock, worker method</verify>
  <done>ParallelCollectionRunner class created with work-stealing queue and shared failure counter</done>
</task>

<task type="auto">
  <name>Task 3: Create ActivityStreamManager and SSE endpoint</name>
  <files>apps/api/src/app/services/activity_stream.py, apps/api/src/app/routers/collection.py, apps/api/src/app/models.py</files>
  <action>
1. Create apps/api/src/app/services/activity_stream.py:

```python
"""Activity stream manager for real-time SSE streaming.

Manages in-memory activity queues keyed by run_id.
Each run has its own asyncio.Queue that workers push to
and SSE endpoints consume from.
"""

import asyncio
import logging
from typing import Any
from weakref import WeakValueDictionary

logger = logging.getLogger(__name__)

# Buffer size per run - older events dropped when full
ACTIVITY_BUFFER_SIZE = 100


class ActivityStreamManager:
    """
    Singleton manager for activity streams.

    Each collection run gets its own asyncio.Queue for activity events.
    Queues are automatically cleaned up when no longer referenced.
    """

    _instance: "ActivityStreamManager | None" = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._streams: dict[str, asyncio.Queue] = {}
            cls._instance._lock = asyncio.Lock()
        return cls._instance

    async def get_or_create(self, run_id: str) -> asyncio.Queue:
        """Get existing queue or create new one for run_id."""
        async with self._lock:
            if run_id not in self._streams:
                self._streams[run_id] = asyncio.Queue(maxsize=ACTIVITY_BUFFER_SIZE)
                logger.debug(f"Created activity stream for run {run_id}")
            return self._streams[run_id]

    async def push(self, run_id: str, event: dict[str, Any]):
        """
        Push event to run's queue.

        If queue is full, drops oldest event to make room.
        """
        queue = await self.get_or_create(run_id)

        # If queue full, drop oldest
        if queue.full():
            try:
                queue.get_nowait()
            except asyncio.QueueEmpty:
                pass

        try:
            queue.put_nowait(event)
        except asyncio.QueueFull:
            # Should not happen after dropping oldest, but handle anyway
            logger.warning(f"Activity queue full for run {run_id}")

    async def cleanup(self, run_id: str):
        """Remove queue for completed run."""
        async with self._lock:
            if run_id in self._streams:
                del self._streams[run_id]
                logger.debug(f"Cleaned up activity stream for run {run_id}")


# Global instance getter
def get_activity_stream() -> ActivityStreamManager:
    """Get the singleton ActivityStreamManager instance."""
    return ActivityStreamManager()
```

2. Add ActivityEntry model to apps/api/src/app/models.py (append to existing file):

```python
# Add this to the existing models.py file

class ActivityEntry(BaseModel):
    """Activity event for SSE streaming."""
    id: str
    timestamp: str
    worker_id: int
    phase: str  # "amazon" or "ebay"
    action: str  # "fetching", "found", "error", "rate_limited", "complete"
    category: str | None = None
    product_name: str | None = None
    seller_found: str | None = None
    new_sellers_count: int | None = None
    error_message: str | None = None
```

3. Add SSE endpoint to apps/api/src/app/routers/collection.py (append after existing endpoints):

```python
# Add these imports at top of file
import json
from fastapi.responses import StreamingResponse
from app.services.activity_stream import get_activity_stream

# Add this endpoint after the /runs/{run_id}/progress endpoint

@router.get("/runs/{run_id}/activity")
async def stream_activity(
    run_id: str,
    user: dict = Depends(require_permission_key("admin.automation")),
    service: CollectionService = Depends(get_collection_service),
):
    """
    Stream real-time activity events for a collection run via SSE.

    Events include:
    - fetching: Worker starting to fetch a category/product
    - found: Sellers found for a product
    - error: Error occurred
    - rate_limited: Waiting due to rate limit
    - complete: Phase or run complete

    Requires admin.automation permission.
    """
    org_id = user["membership"]["org_id"]

    # Verify run exists and belongs to org
    run = await service.get_run(run_id, org_id)
    if not run:
        raise HTTPException(status_code=404, detail="Run not found")

    stream_manager = get_activity_stream()
    queue = await stream_manager.get_or_create(run_id)

    async def event_generator():
        """Generate SSE events from activity queue."""
        while True:
            try:
                # Wait for event with timeout for keepalive
                event = await asyncio.wait_for(queue.get(), timeout=15.0)
                yield f"data: {json.dumps(event)}\n\n"
            except asyncio.TimeoutError:
                # Send keepalive comment
                yield ": keepalive\n\n"
            except asyncio.CancelledError:
                # Client disconnected
                break
            except Exception as e:
                logger.error(f"Activity stream error: {e}")
                break

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        },
    )
```

Note: The SSE endpoint uses query param auth implicitly via the existing Bearer token pattern. EventSource will need token passed differently on frontend (handled in Plan 03).
  </action>
  <verify>
- apps/api/src/app/services/activity_stream.py exists with ActivityStreamManager class
- apps/api/src/app/models.py contains ActivityEntry class
- apps/api/src/app/routers/collection.py has /runs/{run_id}/activity endpoint with StreamingResponse
  </verify>
  <done>ActivityStreamManager created, ActivityEntry model added, SSE endpoint implemented</done>
</task>

</tasks>

<verification>
- [ ] Migration 045_seller_snapshots.sql exists with seller_count_snapshot columns
- [ ] parallel_runner.py exports ParallelCollectionRunner with asyncio.Queue
- [ ] activity_stream.py exports ActivityStreamManager and get_activity_stream
- [ ] collection.py has SSE endpoint at /runs/{run_id}/activity
- [ ] models.py has ActivityEntry model
- [ ] No syntax errors in Python files (run `python -m py_compile` on each)
</verification>

<success_criteria>
1. Migration file ready to add seller_count_snapshot columns
2. ParallelCollectionRunner can distribute tasks to 5 workers via asyncio.Queue
3. ActivityStreamManager manages per-run activity queues
4. SSE endpoint streams events with keepalive pings
5. All Python files pass syntax check
</success_criteria>

<output>
After completion, create `.planning/phases/12-live-activity-feed-concurrency/12-01-SUMMARY.md`
</output>
