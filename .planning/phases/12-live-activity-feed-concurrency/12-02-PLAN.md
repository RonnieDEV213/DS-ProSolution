---
phase: 12-live-activity-feed-concurrency
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - apps/api/src/app/services/collection.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Amazon collection runs with 5 parallel workers"
    - "eBay seller search runs with 5 parallel workers"
    - "Activity events emitted for each worker action"
    - "Seller count snapshot stored on run completion"
    - "Seller count snapshot stored on manual edit operations"
  artifacts:
    - path: "apps/api/src/app/services/collection.py"
      provides: "Parallel collection execution with activity streaming"
      contains: "ParallelCollectionRunner"
  key_links:
    - from: "apps/api/src/app/services/collection.py"
      to: "apps/api/src/app/services/parallel_runner.py"
      via: "import ParallelCollectionRunner"
      pattern: "from app.services.parallel_runner import"
    - from: "apps/api/src/app/services/collection.py"
      to: "apps/api/src/app/services/activity_stream.py"
      via: "import get_activity_stream"
      pattern: "from app.services.activity_stream import"
---

<objective>
Refactor collection execution to use parallel workers and emit activity events for SSE streaming.

Purpose: Replace sequential Amazon and eBay collection with 5-worker parallel execution while streaming activity events.
Output: Updated collection.py with parallel execution and seller snapshot storage.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-live-activity-feed-concurrency/12-CONTEXT.md
@.planning/phases/12-live-activity-feed-concurrency/12-RESEARCH.md
@.planning/phases/12-live-activity-feed-concurrency/12-01-SUMMARY.md
@apps/api/src/app/services/collection.py
@apps/api/src/app/services/parallel_runner.py
@apps/api/src/app/services/activity_stream.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add imports and helper methods for parallel execution</name>
  <files>apps/api/src/app/services/collection.py</files>
  <action>
Add imports and helper methods to collection.py for parallel execution and activity streaming.

1. Add new imports near top of file (after existing imports):

```python
from app.services.parallel_runner import (
    ParallelCollectionRunner,
    create_activity_event,
    CollectionPausedException,
)
from app.services.activity_stream import get_activity_stream
```

2. Add helper method to CollectionService class (after existing helper methods like _normalize_seller_name):

```python
async def _get_seller_count_snapshot(self, org_id: str) -> int:
    """Get current seller count for snapshot storage."""
    result = (
        self.supabase.table("sellers")
        .select("id", count="exact")
        .eq("org_id", org_id)
        .execute()
    )
    return result.count or 0

async def _store_run_snapshot(self, run_id: str, org_id: str):
    """Store seller count snapshot on run completion."""
    count = await self._get_seller_count_snapshot(org_id)
    self.supabase.table("collection_runs").update({
        "seller_count_snapshot": count,
    }).eq("id", run_id).execute()
    logger.info(f"Stored seller snapshot {count} for run {run_id}")
```

3. Update the _log_seller_change method to include seller_count_snapshot:

Find the _log_seller_change method and modify it to accept and store seller_count_snapshot.

Add parameter `seller_count_snapshot: int | None = None` to the method signature.

In the log_data dict being inserted, add:
```python
"seller_count_snapshot": seller_count_snapshot,
```

4. Update add_seller, update_seller, remove_seller, bulk_add_sellers, bulk_remove_sellers to capture and store snapshot:

For each of these methods, after the database operation completes successfully, get and pass the snapshot:

```python
# Get current seller count for snapshot
seller_count = await self._get_seller_count_snapshot(org_id)

# Pass to log method
await self._log_seller_change(
    # ... existing args ...
    seller_count_snapshot=seller_count,
)
```

Note: For bulk operations, get the count AFTER the bulk operation completes to reflect the final state.
  </action>
  <verify>
- collection.py imports ParallelCollectionRunner and get_activity_stream
- _get_seller_count_snapshot method exists
- _store_run_snapshot method exists
- _log_seller_change accepts seller_count_snapshot parameter
  </verify>
  <done>Imports added, helper methods created, audit log updated to store snapshots</done>
</task>

<task type="auto">
  <name>Task 2: Refactor run_amazon_collection to use parallel workers</name>
  <files>apps/api/src/app/services/collection.py</files>
  <action>
Refactor run_amazon_collection method to use ParallelCollectionRunner with 5 workers.

Replace the sequential for-loop with parallel execution. Key changes:

1. Create activity stream callback at start of method:

```python
activity_manager = get_activity_stream()

def emit_activity(event):
    """Push activity event to SSE stream."""
    import asyncio
    asyncio.create_task(activity_manager.push(run_id, event.to_dict()))
```

2. Create ParallelCollectionRunner:

```python
runner = ParallelCollectionRunner(
    max_workers=5,
    on_activity=emit_activity,
)
```

3. Define the task processing function that handles a single category:

```python
async def process_category(task: dict, worker_id: int) -> dict:
    """Process a single category - called by parallel worker."""
    cat_id = task["cat_id"]
    node_id = task["node_id"]
    category_name = task["category_name"]

    # Emit fetching activity
    await runner.emit_activity(create_activity_event(
        worker_id=worker_id,
        phase="amazon",
        action="fetching",
        category=category_name,
    ))

    # Check if run was cancelled
    run_check = await self.get_run(run_id, org_id)
    if run_check and run_check["status"] in ("cancelled", "paused"):
        raise CollectionPausedException(f"Run {run_check['status']}")

    # Retry logic (3 attempts)
    for attempt in range(3):
        result = await scraper.fetch_bestsellers(node_id, category_name=category_name)

        if result.error == "rate_limited":
            await runner.emit_activity(create_activity_event(
                worker_id=worker_id,
                phase="amazon",
                action="rate_limited",
                category=category_name,
            ))
            await asyncio.sleep(5)
            continue

        if result.error:
            await runner.emit_activity(create_activity_event(
                worker_id=worker_id,
                phase="amazon",
                action="error",
                category=category_name,
                error_message=result.error,
            ))
            raise Exception(result.error)

        # Success - emit found event
        await runner.emit_activity(create_activity_event(
            worker_id=worker_id,
            phase="amazon",
            action="found",
            category=category_name,
            new_sellers_count=len(result.products),  # Reusing field for product count
        ))

        return {
            "cat_id": cat_id,
            "products": result.products,
            "category_name": category_name,
        }

    # Max retries reached
    return {"cat_id": cat_id, "products": [], "error": "max_retries"}
```

4. Prepare tasks list:

```python
tasks = []
for cat_id in category_ids:
    node_id = node_lookup.get(cat_id)
    if not node_id:
        continue
    tasks.append({
        "cat_id": cat_id,
        "node_id": node_id,
        "category_name": name_lookup.get(cat_id, cat_id),
    })
```

5. Execute parallel:

```python
try:
    results = await runner.run(tasks, process_category, phase="amazon")
except CollectionPausedException:
    # Handle pause
    await self.pause_run(run_id, org_id)
    return {"status": "paused", "products_fetched": products_fetched}
```

6. Process results and save to database:

```python
for result in results:
    if "error" in result:
        errors.append({"category": result["cat_id"], "error": result["error"]})
        continue

    products_fetched += len(result["products"])

    # Save products as collection items (batch for efficiency)
    items_to_insert = []
    for product in result["products"]:
        items_to_insert.append({
            "run_id": run_id,
            "item_type": "amazon_product",
            "external_id": product.asin,
            "data": {
                "title": product.title,
                "price": product.price,
                "currency": product.currency,
                "rating": product.rating,
                "url": product.url,
                "position": product.position,
                "category_id": result["cat_id"],
            },
            "status": "pending",
        })

    if items_to_insert:
        batched_insert(self.supabase, table="collection_items", rows=items_to_insert)

# Update progress after all workers complete
self.supabase.table("collection_runs").update({
    "departments_completed": departments_total,
    "categories_completed": categories_total,
    "products_total": products_fetched,
    # ... other updates
}).eq("id", run_id).execute()
```

7. Emit phase complete activity:

```python
await activity_manager.push(run_id, {
    "id": str(uuid.uuid4()),
    "timestamp": datetime.now(timezone.utc).isoformat(),
    "worker_id": 0,  # 0 = system message
    "phase": "amazon",
    "action": "complete",
})
```

Keep the same overall structure but replace the sequential loop with parallel execution. Maintain checkpoint logic for resume support.
  </action>
  <verify>
- run_amazon_collection uses ParallelCollectionRunner
- Activity events emitted for fetching, found, error, rate_limited, complete
- Products saved to collection_items after parallel execution
- Print statements remain for terminal output (in addition to activity events)
  </verify>
  <done>Amazon collection refactored to use 5 parallel workers with activity streaming</done>
</task>

<task type="auto">
  <name>Task 3: Refactor run_ebay_seller_search to use parallel workers and store snapshot</name>
  <files>apps/api/src/app/services/collection.py</files>
  <action>
Refactor run_ebay_seller_search method to use ParallelCollectionRunner and store seller snapshot on completion.

Similar pattern to Amazon but for eBay search:

1. Create activity stream and runner at method start:

```python
activity_manager = get_activity_stream()

def emit_activity(event):
    import asyncio
    asyncio.create_task(activity_manager.push(run_id, event.to_dict()))

runner = ParallelCollectionRunner(
    max_workers=5,
    on_activity=emit_activity,
)
```

2. Define task processing function for single product:

```python
async def process_product(task: dict, worker_id: int) -> dict:
    """Process a single product - search eBay and extract sellers."""
    product = task["product"]
    product_data = product.get("data", {})
    title = product_data.get("title")
    price = product_data.get("price")
    cat_id = product_data.get("category_id", "unknown")
    cat_name = cat_name_lookup.get(cat_id, cat_id.replace("-", " ").title())

    if not title or not price:
        return {"product_id": product["id"], "sellers": [], "skipped": True}

    # Parse price
    if isinstance(price, str):
        try:
            price = float(price.replace("$", "").replace(",", ""))
        except ValueError:
            return {"product_id": product["id"], "sellers": [], "skipped": True}

    short_title = title[:40] + "..." if len(title) > 40 else title

    # Emit fetching activity
    await runner.emit_activity(create_activity_event(
        worker_id=worker_id,
        phase="ebay",
        action="fetching",
        category=cat_name,
        product_name=short_title,
    ))

    all_sellers = []

    # Search 3 pages per product
    for page in range(1, 4):
        # Check cancellation
        run_check = await self.get_run(run_id, org_id)
        if run_check and run_check["status"] in ("cancelled", "paused"):
            raise CollectionPausedException(f"Run {run_check['status']}")

        result = await scraper.search_sellers(title, price, page)

        if result.error == "rate_limited":
            await runner.emit_activity(create_activity_event(
                worker_id=worker_id,
                phase="ebay",
                action="rate_limited",
                product_name=short_title,
            ))
            await asyncio.sleep(5)
            continue

        if result.error:
            break  # Skip remaining pages

        all_sellers.extend(result.sellers)

        if not result.has_more:
            break

        await asyncio.sleep(0.2)  # 200ms delay between pages

    # Emit found activity
    if all_sellers:
        await runner.emit_activity(create_activity_event(
            worker_id=worker_id,
            phase="ebay",
            action="found",
            category=cat_name,
            product_name=short_title,
            new_sellers_count=len(all_sellers),
        ))

    return {
        "product_id": product["id"],
        "cat_id": cat_id,
        "sellers": all_sellers,
    }
```

3. Prepare tasks:

```python
tasks = [{"product": p} for p in products]
```

4. Execute parallel:

```python
try:
    results = await runner.run(tasks, process_product, phase="ebay")
except CollectionPausedException:
    await self.pause_run(run_id, org_id)
    return {"status": "paused", "sellers_found": sellers_found, "sellers_new": sellers_new}
```

5. Process results - dedupe and save sellers:

After parallel execution, batch process all sellers:

```python
# Collect all sellers from results
all_found_sellers = []
for result in results:
    if result.get("skipped"):
        continue
    all_found_sellers.extend(result.get("sellers", []))

sellers_found = len(all_found_sellers)

# Batch dedupe and insert (use existing batched operations)
# ... similar to existing code but processing all at once ...
```

6. At run completion, store seller snapshot:

```python
# Mark run as completed
now = datetime.now(timezone.utc).isoformat()
self.supabase.table("collection_runs").update({
    "status": "completed",
    "completed_at": now,
    "products_searched": len(products),
    "sellers_found": sellers_found,
    "sellers_new": sellers_new,
    "updated_at": now,
}).eq("id", run_id).execute()

# Store seller count snapshot
await self._store_run_snapshot(run_id, org_id)

# Cleanup activity stream
await activity_manager.cleanup(run_id)
```

7. Emit complete activity:

```python
await activity_manager.push(run_id, {
    "id": str(uuid.uuid4()),
    "timestamp": datetime.now(timezone.utc).isoformat(),
    "worker_id": 0,
    "phase": "ebay",
    "action": "complete",
    "new_sellers_count": sellers_new,
})
```

Key differences from current implementation:
- Products processed in parallel by 5 workers
- Activity events emitted per worker action
- Seller snapshot stored on completion
- Activity stream cleaned up on completion
  </action>
  <verify>
- run_ebay_seller_search uses ParallelCollectionRunner
- Activity events emitted for fetching, found, rate_limited
- _store_run_snapshot called at run completion
- activity_manager.cleanup called at end
- Existing terminal print statements preserved
  </verify>
  <done>eBay search refactored to 5 parallel workers with activity streaming and seller snapshot storage</done>
</task>

</tasks>

<verification>
- [ ] collection.py imports ParallelCollectionRunner and get_activity_stream
- [ ] _get_seller_count_snapshot and _store_run_snapshot methods exist
- [ ] _log_seller_change stores seller_count_snapshot
- [ ] add_seller, update_seller, remove_seller, bulk_add_sellers, bulk_remove_sellers pass snapshot to log
- [ ] run_amazon_collection uses ParallelCollectionRunner with 5 workers
- [ ] run_ebay_seller_search uses ParallelCollectionRunner with 5 workers
- [ ] Activity events emitted during collection
- [ ] Seller snapshot stored on run completion
- [ ] Activity stream cleaned up on completion
- [ ] No syntax errors: `python -m py_compile apps/api/src/app/services/collection.py`
</verification>

<success_criteria>
1. Amazon collection executes categories in parallel (5 workers)
2. eBay search executes products in parallel (5 workers)
3. Activity events stream to SSE endpoint during collection
4. Seller count snapshot stored when run completes
5. Seller count snapshot stored on manual add/edit/remove operations
6. Terminal output preserved for debugging
</success_criteria>

<output>
After completion, create `.planning/phases/12-live-activity-feed-concurrency/12-02-SUMMARY.md`
</output>
