---
phase: 28-collection-storage-rendering-infrastructure
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/api/src/app/routers/export.py
  - apps/api/src/app/services/export_service.py
autonomous: true

must_haves:
  truths:
    - "GET /export/sellers/csv returns a streaming CSV download of sellers"
    - "GET /export/sellers/json returns a streaming JSON download of sellers"
    - "Both endpoints accept optional flagged filter query parameter"
    - "Both endpoints require seller_collection.read permission"
  artifacts:
    - path: "apps/api/src/app/routers/export.py"
      provides: "Seller streaming export endpoints"
      contains: "/sellers/csv"
    - path: "apps/api/src/app/services/export_service.py"
      provides: "Seller CSV/JSON stream generators"
      contains: "generate_sellers_csv_stream"
  key_links:
    - from: "apps/api/src/app/routers/export.py"
      to: "apps/api/src/app/services/export_service.py"
      via: "generate_sellers_csv_stream, generate_sellers_json_stream"
      pattern: "generate_sellers_"
---

<objective>
Add streaming CSV and JSON export endpoints for sellers to the existing export router.

Purpose: The current SellersGrid only has client-side export. For datasets exceeding 10K sellers (projected 50-100K+), server-side streaming export prevents browser memory exhaustion. These endpoints mirror the existing records export pattern.

Output: Two new GET endpoints in export.py, two new generator functions in export_service.py.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/28-collection-storage-rendering-infrastructure/28-CONTEXT.md
@.planning/phases/28-collection-storage-rendering-infrastructure/28-RESEARCH.md
@apps/api/src/app/routers/export.py
@apps/api/src/app/services/export_service.py
@apps/api/src/app/routers/sellers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add seller stream generators to export_service.py</name>
  <files>
    apps/api/src/app/services/export_service.py
  </files>
  <action>
Add two new async generator functions to export_service.py, following the existing `generate_csv_stream` and `generate_json_stream` patterns but for sellers:

1. `generate_sellers_csv_stream(supabase, org_id: str, flagged: Optional[bool] = None) -> AsyncGenerator[str, None]`:
   - CSV headers: `display_name,normalized_name,platform,platform_id,times_seen,feedback_percent,feedback_count,flagged,created_at`
   - Yield header row first.
   - Fetch sellers from `collection_sellers` table with cursor pagination (batch size 100).
   - Filter by `org_id`. If `flagged` is not None, filter by `flagged` column.
   - Order by `created_at DESC` for consistent export order.
   - For each batch, yield CSV rows using csv.writer with StringIO buffer.
   - Use the same cursor pagination pattern as the existing records export:
     ```python
     query = supabase.table("collection_sellers").select("*").eq("org_id", org_id)
     if flagged is not None:
         query = query.eq("flagged", flagged)
     query = query.order("created_at", desc=True).limit(EXPORT_BATCH_SIZE)
     ```
   - Continue fetching until batch returns fewer than EXPORT_BATCH_SIZE rows.

2. `generate_sellers_json_stream(supabase, org_id: str, flagged: Optional[bool] = None) -> AsyncGenerator[str, None]`:
   - Yield opening `{"exported_at": "...", "sellers": [`.
   - For each seller, yield JSON object (comma-separated after first).
   - Yield closing `]}`.
   - Same pagination and filtering as CSV.

Important: The existing export_service uses `account_id` for records. Sellers use `org_id` instead (sellers are org-wide, not per-account). Make sure the query uses `org_id` not `account_id`.

Seller columns from DB: `id, org_id, display_name, normalized_name, platform, platform_id, times_seen, feedback_percent, feedback_count, first_seen_run_id, last_seen_run_id, flagged, created_at, updated_at, deleted_at`. Export should exclude internal columns (id, org_id, first_seen_run_id, last_seen_run_id, updated_at, deleted_at) and include the user-facing ones listed in the CSV headers above.
  </action>
  <verify>
    - Python syntax check: `cd apps/api && python -c "from app.services.export_service import generate_sellers_csv_stream, generate_sellers_json_stream"`
    - Both functions are importable.
  </verify>
  <done>
    - generate_sellers_csv_stream produces valid CSV with header + data rows.
    - generate_sellers_json_stream produces valid JSON with sellers array.
    - Both accept optional flagged filter.
    - Both use cursor pagination (batch size 100) for memory efficiency.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add seller export endpoints to export router</name>
  <files>
    apps/api/src/app/routers/export.py
  </files>
  <action>
Add two new streaming export endpoints to the existing export router, placed after the records export section but before the background export section:

1. `GET /export/sellers/csv`:
   ```python
   @router.get("/sellers/csv")
   async def export_sellers_csv(
       flagged: Optional[bool] = Query(None, description="Filter by flagged status"),
       user: dict = Depends(require_permission_key("seller_collection.read")),
   ):
       """Stream CSV export of sellers."""
       supabase = get_supabase_for_user(user["token"])
       org_id = user["membership"]["org_id"]
       date_str = datetime.now().strftime("%Y%m%d")
       filename = f"sellers_{date_str}.csv"

       return StreamingResponse(
           generate_sellers_csv_stream(supabase, org_id, flagged=flagged),
           media_type="text/csv",
           headers={
               "Content-Disposition": f'attachment; filename="{filename}"',
               "Cache-Control": "no-cache",
           },
       )
   ```

2. `GET /export/sellers/json`:
   ```python
   @router.get("/sellers/json")
   async def export_sellers_json(
       flagged: Optional[bool] = Query(None, description="Filter by flagged status"),
       user: dict = Depends(require_permission_key("seller_collection.read")),
   ):
       """Stream JSON export of sellers."""
       supabase = get_supabase_for_user(user["token"])
       org_id = user["membership"]["org_id"]
       date_str = datetime.now().strftime("%Y%m%d")
       filename = f"sellers_{date_str}.json"

       return StreamingResponse(
           generate_sellers_json_stream(supabase, org_id, flagged=flagged),
           media_type="application/json",
           headers={
               "Content-Disposition": f'attachment; filename="{filename}"',
               "Cache-Control": "no-cache",
           },
       )
   ```

3. Add import for `generate_sellers_csv_stream, generate_sellers_json_stream` from export_service.

Key differences from records export:
- Permission: `seller_collection.read` (not `order_tracking.read`)
- No `account_id` parameter (sellers are org-wide)
- Only `flagged` filter (no date/status/columns filters -- sellers are simpler)
- Filename uses "sellers_" prefix
  </action>
  <verify>
    - Python syntax check: `cd apps/api && python -c "from app.routers.export import router"`
    - Both endpoints are registered on the router.
    - Grep confirms `/sellers/csv` and `/sellers/json` in export.py.
  </verify>
  <done>
    - GET /export/sellers/csv returns StreamingResponse with CSV content type.
    - GET /export/sellers/json returns StreamingResponse with JSON content type.
    - Both require seller_collection.read permission.
    - Both accept optional flagged boolean query parameter.
    - Existing records export endpoints unchanged.
  </done>
</task>

</tasks>

<verification>
- Python imports work: `cd apps/api && python -c "from app.routers.export import router; from app.services.export_service import generate_sellers_csv_stream, generate_sellers_json_stream"`
- No existing endpoint behavior changed.
- Both new endpoints follow the established streaming pattern.
</verification>

<success_criteria>
- Two new streaming export endpoints for sellers (CSV, JSON).
- Endpoints use cursor pagination for memory efficiency.
- Permission gating matches seller_collection.read.
- Existing records export endpoints fully preserved.
</success_criteria>

<output>
After completion, create `.planning/phases/28-collection-storage-rendering-infrastructure/28-02-SUMMARY.md`
</output>
