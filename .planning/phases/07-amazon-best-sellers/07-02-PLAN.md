---
phase: 07-amazon-best-sellers
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - apps/api/src/app/services/scrapers/oxylabs.py
  - apps/api/src/app/services/scrapers/__init__.py
  - apps/api/pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Oxylabs scraper can fetch Amazon Best Sellers by browse node ID"
    - "Rate limit (429) responses are detected and signaled"
    - "API cost per request is tracked in cents"
    - "Credentials are read from environment variables"
  artifacts:
    - path: "apps/api/src/app/services/scrapers/oxylabs.py"
      provides: "OxylabsAmazonScraper implementation"
      exports: ["OxylabsAmazonScraper", "COST_PER_BESTSELLERS_PAGE_CENTS"]
    - path: "apps/api/pyproject.toml"
      provides: "httpx dependency"
      contains: "httpx"
  key_links:
    - from: "oxylabs.py"
      to: "base.py"
      via: "class inheritance"
      pattern: "class OxylabsAmazonScraper\\(AmazonScraperService\\)"
    - from: "oxylabs.py"
      to: "Oxylabs API"
      via: "httpx POST request"
      pattern: "realtime\\.oxylabs\\.io"
---

<objective>
Implement Oxylabs E-Commerce Scraper API integration for fetching Amazon Best Sellers products.

Purpose: Enable actual product fetching from Amazon via Oxylabs API. This is the core scraping capability that the collection pipeline will use.

Output: Working OxylabsAmazonScraper class that can fetch products by category node ID
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-amazon-best-sellers/07-RESEARCH.md
@.planning/phases/07-amazon-best-sellers/07-CONTEXT.md
@apps/api/src/app/services/scrapers/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add httpx dependency</name>
  <files>apps/api/pyproject.toml</files>
  <action>
Add httpx to the project dependencies. httpx is already in dev dependencies - move it to main dependencies for production use.

In pyproject.toml, add to the `dependencies` array:
```
"httpx>=0.26.0",
```

Note: httpx is already in dev dependencies, but we need it in production for Oxylabs API calls.
  </action>
  <verify>`grep -q '"httpx' apps/api/pyproject.toml` shows httpx in dependencies</verify>
  <done>httpx>=0.26.0 is in main dependencies array</done>
</task>

<task type="auto">
  <name>Task 2: Implement OxylabsAmazonScraper</name>
  <files>apps/api/src/app/services/scrapers/oxylabs.py, apps/api/src/app/services/scrapers/__init__.py</files>
  <action>
Create the Oxylabs implementation of AmazonScraperService.

1. Create `oxylabs.py`:

```python
"""Oxylabs E-Commerce Scraper API implementation.

Uses Oxylabs amazon_bestsellers source to fetch structured product data.
Credentials: OXYLABS_USERNAME and OXYLABS_PASSWORD environment variables.
"""

import logging
import os

import httpx

from .base import AmazonProduct, AmazonScraperService, ScrapeResult

logger = logging.getLogger(__name__)

# Cost per bestsellers page request in cents (~$0.03 per page, ~50 products)
COST_PER_BESTSELLERS_PAGE_CENTS = 3


class OxylabsAmazonScraper(AmazonScraperService):
    """Oxylabs E-Commerce API implementation for Amazon Best Sellers."""

    def __init__(self):
        self.username = os.environ.get("OXYLABS_USERNAME")
        self.password = os.environ.get("OXYLABS_PASSWORD")
        self.base_url = "https://realtime.oxylabs.io/v1/queries"

        if not self.username or not self.password:
            raise ValueError("OXYLABS_USERNAME and OXYLABS_PASSWORD environment variables required")

    async def fetch_bestsellers(
        self,
        category_node_id: str,
        page: int = 1,
    ) -> ScrapeResult:
        """Fetch best sellers for a category from Oxylabs API."""
        payload = {
            "source": "amazon_bestsellers",
            "domain": "com",
            "query": category_node_id,
            "parse": True,
            "start_page": page,
            "pages": 1,
        }

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    self.base_url,
                    json=payload,
                    auth=(self.username, self.password),
                    timeout=30.0,
                )

            # Handle rate limiting
            if response.status_code == 429:
                logger.warning(f"Rate limited on category {category_node_id}")
                return ScrapeResult(
                    products=[],
                    cost_cents=0,
                    page=page,
                    total_pages=None,
                    error="rate_limited",
                )

            response.raise_for_status()
            data = response.json()

            # Parse response structure
            results = data.get("results", [])
            if not results:
                return ScrapeResult(
                    products=[],
                    cost_cents=COST_PER_BESTSELLERS_PAGE_CENTS,
                    page=page,
                    total_pages=None,
                    error="empty_response",
                )

            content = results[0].get("content", {})
            raw_products = content.get("results", [])

            # Parse products
            products = []
            for p in raw_products:
                try:
                    product = AmazonProduct(
                        asin=p.get("asin", ""),
                        title=p.get("title", ""),
                        price=p.get("price"),
                        currency=p.get("currency", "USD"),
                        rating=p.get("rating"),
                        url=p.get("url", ""),
                        position=p.get("pos", 0),
                    )
                    products.append(product)
                except Exception as e:
                    logger.warning(f"Failed to parse product: {e}")
                    continue

            logger.info(f"Fetched {len(products)} products from category {category_node_id}")

            return ScrapeResult(
                products=products,
                cost_cents=COST_PER_BESTSELLERS_PAGE_CENTS,
                page=page,
                total_pages=None,  # Oxylabs doesn't report total pages
                error=None,
            )

        except httpx.TimeoutException:
            logger.error(f"Timeout fetching category {category_node_id}")
            return ScrapeResult(
                products=[],
                cost_cents=0,
                page=page,
                total_pages=None,
                error="timeout",
            )
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP error fetching category {category_node_id}: {e}")
            return ScrapeResult(
                products=[],
                cost_cents=0,
                page=page,
                total_pages=None,
                error=f"http_error:{e.response.status_code}",
            )
        except Exception as e:
            logger.error(f"Unexpected error fetching category {category_node_id}: {e}")
            return ScrapeResult(
                products=[],
                cost_cents=0,
                page=page,
                total_pages=None,
                error=str(e),
            )
```

2. Update `__init__.py` to export the Oxylabs implementation:
```python
from .base import AmazonProduct, AmazonScraperService, ScrapeResult
from .oxylabs import COST_PER_BESTSELLERS_PAGE_CENTS, OxylabsAmazonScraper

__all__ = [
    "AmazonProduct",
    "ScrapeResult",
    "AmazonScraperService",
    "OxylabsAmazonScraper",
    "COST_PER_BESTSELLERS_PAGE_CENTS",
]
```

Key implementation details:
- Uses httpx.AsyncClient for async HTTP
- Basic auth with username:password
- Returns structured ScrapeResult with error handling
- Logs warnings for rate limits and parsing errors
- Cost tracking per request (3 cents = ~$0.03)
  </action>
  <verify>Import succeeds: `cd apps/api/src && python -c "from app.services.scrapers import OxylabsAmazonScraper"`</verify>
  <done>OxylabsAmazonScraper class implements fetch_bestsellers with rate limit and error handling</done>
</task>

</tasks>

<verification>
1. httpx is in pyproject.toml dependencies
2. OxylabsAmazonScraper imports without error
3. Class inherits from AmazonScraperService
4. fetch_bestsellers method is async and returns ScrapeResult
5. All files committed to git
</verification>

<success_criteria>
- [ ] httpx added to main dependencies in pyproject.toml
- [ ] OxylabsAmazonScraper class exists and is importable
- [ ] fetch_bestsellers handles 429 rate limit responses
- [ ] COST_PER_BESTSELLERS_PAGE_CENTS exported (value: 3)
</success_criteria>

<output>
After completion, create `.planning/phases/07-amazon-best-sellers/07-02-SUMMARY.md`
</output>
